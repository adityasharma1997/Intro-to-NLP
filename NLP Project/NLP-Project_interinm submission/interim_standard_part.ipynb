{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An8kDHQklTPH",
        "outputId": "90612478-9920-4833-bbb0-223d6fdc0080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.2\n"
          ]
        }
      ],
      "source": [
        "from io import open\n",
        "!pip install conllu\n",
        "from conllu import parse_incr\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim"
      ],
      "metadata": {
        "id": "AUs_EM1SlWJv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-17qLztTlYs9",
        "outputId": "3fe2a21e-5cee-4069-b38d-380aa359f49a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.corpus\n"
      ],
      "metadata": {
        "id": "ZJNJQypKlbM3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_name):\n",
        "    data_file = open(file_name, \"r\", encoding=\"utf-8\")\n",
        "    # get 0th, 2nd and 4th columns of different sentences\n",
        "    sentences = []\n",
        "    sentence_tags = []\n",
        "    for tokenlist in parse_incr(data_file):\n",
        "        curr_sentence = []\n",
        "        curr_tags = []\n",
        "        for token in tokenlist:\n",
        "            sen=token[\"form\"]\n",
        "            curr_sentence.append(sen)\n",
        "            tag=token[\"upostag\"] \n",
        "            curr_tags.append(tag)\n",
        "        sentences.append(curr_sentence)\n",
        "        sentence_tags.append(curr_tags)\n",
        "    return sentences, sentence_tags\n",
        "train_filepath=\"/content/drive/MyDrive/NLP_ASSG2/en_atis-ud-train.conllu\"\n",
        "test_filepath=\"/content/drive/MyDrive/NLP_ASSG2/en_atis-ud-test.conllu\"\n",
        "train_sentences, train_tags = load_data(train_filepath)\n",
        "test_sentences, test_tags = load_data(test_filepath)"
      ],
      "metadata": {
        "id": "znfwDu2Vlh4f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "class DependencyParser:\n",
        "    def __init__(self, grammar):\n",
        "        self.grammar = grammar\n",
        "    \n",
        "    def parse(self, sentence):\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        parser = nltk.parse.ShiftReduceParser(self.grammar)\n",
        "        tree = None\n",
        "        \n",
        "        try:\n",
        "            for tree in parser.parse(tokens):\n",
        "                pass\n",
        "        except ValueError:\n",
        "            return None\n",
        "        \n",
        "        if tree is None:\n",
        "            return None\n",
        "        \n",
        "        dependencies = []\n",
        "        for node in tree.nodes.values():\n",
        "            if node[\"address\"] == 0:\n",
        "                continue\n",
        "            governor = tokens[node[\"head\"]]\n",
        "            dependent = tokens[node[\"address\"] - 1]\n",
        "            dependencies.append((governor, dependent, node[\"rel\"]))\n",
        "        \n",
        "        return dependencies\n"
      ],
      "metadata": {
        "id": "RmftpNbplmDR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
        "'saw' -> 'I' | 'her' | 'the' | 'with'\n",
        "'with' -> 'telescope'\n",
        "'telescope' -> 'my'\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "BZPvNwu-ltLA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFbqbRc0l4wu",
        "outputId": "9fe7a5d5-8809-49b1-f948-8335fb14ad8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = DependencyParser(grammar)\n",
        "sentence = \"I saw her with my telescope\"\n",
        "dependencies = parser.parse(sentence)\n",
        "print(dependencies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjxNaaGClwSx",
        "outputId": "0dac87ae-cf7a-4f21-ccf4-acfecf0fb78b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('saw', 'I', 'nsubj'), ('saw', 'her', 'dobj'), ('saw', 'telescope', 'prep'), ('telescope', 'my', 'poss')]\n"
          ]
        }
      ]
    }
  ]
}