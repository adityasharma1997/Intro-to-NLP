# -*- coding: utf-8 -*-
"""2021201016_Assg_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mJ2x9VSqQvbeCtrfXjchgVpxcRUscv3F
"""

from google.colab import drive
drive.mount('/content/drive')

import re
import json
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD
from scipy import spatial
from numpy import dot
from numpy.linalg import norm
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import matplotlib.cm as cm

path ="/content/drive/MyDrive/nlp/reviews_Movies_and_TV.json"

def tokenize(data):
  data=str(data)
  # make the text lowercase
  data = data.lower()

  # removing punctuations
  data = re.sub(r'(\w+)([.,!?:;\[\]*/"\'\(\)])', r'\1 \2', data)
  data = re.sub(r'([.,!?:;\[\]*/"\'\(\)])(\w+)', r'\1 \2', data)
  
  
  #removing extra spaces
  data = re.sub(r'\s+', ' ', data)

  # split words with hyphens
  data = re.sub(r'(\w+)-(\w+)', r'\1 \2', data)

  return data

data=[]
count=0
with open(path) as fd:
  for line in fd:
    count=count+1
    data.append(tokenize((json.loads(line))['reviewText']))
    if count > 50000:
      break

def make_vocab(data):
  vocab=list()
  vocab_freq=dict()
  for sentence in data:
    spaced_data=sentence.split()
    for words in spaced_data:
      if words in vocab_freq:
        vocab_freq[words]+=1
      else:
        vocab.append(words)
        vocab_freq[words]=1
 
  #handling unknowns
  vocab.append('UNK')
  vocab_freq['UNK']=0
  #removing those words which doesn't occur many times in the vocab
  for item,values in vocab_freq.items():
    if values<3:
      vocab.remove(item)
      vocab_freq['UNK']+=values
      # del vocab_freq[item]
  return vocab,vocab_freq

def word_to_index(vocab_freq):
  word2idx=dict()
  idx2word=dict()
  c=0
  for i,val in vocab_freq.items():
    word2idx[i]=c
    idx2word[c]=i
    c+=1
  return word2idx,idx2word

#
vocab,vocab_freq=make_vocab(data)
# print(vocab_freq)
# print(vocab)
len(vocab)

word2idx,idx2word=word_to_index(vocab_freq)

def comatrix(vocab,data,window,vocab_freq):
  n=len(vocab)
  size=(n,n)
  matrix=np.zeros(size,dtype='int16')
  for sentence in data:
    words=sentence.split()
    #marking all the words which which occur less than significant times as unknown
    for i in range(0,len(words)):
      if vocab_freq[words[i]]<4:
        words[i]='UNK'
    for i in range(0,len(words)):
      for j in range(1,window+1):
        if(i+j<len(words)):
          matrix[vocab.index(words[i]),vocab.index(words[i+j])]+=1
        if(i-j>=0):
          matrix[vocab.index(words[i]),vocab.index(words[i-j])]+=1
  return matrix

#window size is context of words which we are taken
window_size=1
matrix=comatrix(vocab,data,window_size,vocab_freq)

matrix.shape

sparse_matrix = csr_matrix(matrix)
sparse_matrix.shape

svd = TruncatedSVD(n_components=50,n_iter=7, random_state=42)
matrix=svd.fit_transform(sparse_matrix)

matrix.shape

matrix[200]

np.savetxt('/content/drive/MyDrive/nlp/modelsvd.csv', matrix)

matrix=np.loadtxt("/content/drive/MyDrive/nlp/modelsvd.csv")

def cosine(f,s):
  res= dot(f,s)/(norm(f)*norm(s))
  if np.isnan(res):
    res=0
  return res
  # return 1 - spatial.distance.cosine(f, s)

def top_ten_words(word):
  index_of_word=vocab.index(word)
  
  #finding the matrix corresponding to that word
  word_matrix=matrix[index_of_word]

  similarity_words=dict()
  for i in range(0,len(vocab)):
    if vocab[i] == word:
      continue
    similar=cosine(matrix[i],word_matrix)
    # print(similar)
    similarity_words[vocab[i]]=similar

  # print(similarity_words)
  sorted_values=sorted(similarity_words.items(), key=lambda x:x[1], reverse=True)
  # print(sorted_values)
  #excluding 0th index as it will be same word and taking top ten
  # print(type(sorted_values))
  
  top_ten=sorted_values[:10]
  # print(top_ten)
  return top_ten

def plot_2D(words):

  plt.figure(figsize=(18, 18)) 

  colors = cm.rainbow(np.linspace(0, 1, 5))
  # 5 will be no of words we are given for each colour we will have different colors

  for word, c in zip(words, colors):
    index_of_word=vocab.index(word)
    embedding=np.array([matrix[index_of_word]])
    # print(embedding.shape)
    all_words=list()
    all_words.append(word)

    top_ten_similar_word=top_ten_words(word)

    for i in top_ten_similar_word:
      all_words.append(i[0])
      index_of_similar_word=vocab.index(i[0])
      embedding=np.append(embedding,[matrix[index_of_similar_word]],axis=0)
    
    
    # print(embedding.shape)

    
    X_embedding=TSNE(n_components=2, learning_rate='auto',init='pca', perplexity=5).fit_transform(embedding)
    
    
    # two=X_embedding[:,:2]
    # print(two)
    
    
    print(all_words)
    plt.scatter(X_embedding[:, 0], X_embedding[:, 1], color = c,label=c)

    for i in range(0,len(all_words)):
      plt.annotate(all_words[i], xy=(X_embedding[i, 0], X_embedding[i, 1]), xytext=(5, 2),
                 textcoords='offset points', 
                 ha='right', 
                 va='bottom')
    plt.title('t-SNE plot of high-dimensional data of ' + all_words[0])
    
    # for word, (x,y) in zip(top_ten_similar_word, twodim):

words = ['titanic']
plot_2D(words)

words = ['great']
plot_2D(words)
# print(t)

words = ['man']
plot_2D(words)

words = ['camera']
plot_2D(words)

words = ['daughter']
plot_2D(words)

words=['great','daughter','is','camera','man']
plot_2D(words)

words = ['is']
plot_2D(words)

"""CBOW"""

import re
import json
import numpy as np
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, InputLayer, Embedding, LSTM, Flatten, GlobalAveragePooling1D
from keras.optimizers import Adam
import keras
import pickle

def get_positive_samples(data,word_to_idx):
    positive_samples = []
    for sentence in data:
      words = sentence.split()
      win_size=1
      for i in range(win_size, len(words) - win_size):
        target = word_to_idx[words[i]]
        context_x=[word_to_idx[words[i-1]],word_to_idx[words[i+1]]]
        context_x.append(target)
        positive_samples.append(context_x)
    return positive_samples

totalWords=0
for freq in vocab_freq.values():
  totalWords+=freq**(3/4)

wordProb=dict()
for word,freq in vocab_freq.items():
  wordProb[word]=(freq/totalWords)**(3/4)

keys=list(wordProb.keys())
s=sum(wordProb.values())
p=list()
for i in wordProb.values():
  p.append(i/s)

def get_negative_samples(word2idx):
    win_size=1
    negative_samples = []
    for _ in range(3):
      wrd= np.random.choice(keys, p=p) 
      idx = word2idx[wrd]
      negative_samples.append(idx)
    return negative_samples

def get_train_x_y(data,word_to_idx):
    win_size=1
    x, y=[],[]
    k = 0
    for sentence in data:
        words = sentence.split()

        for i in range(win_size, len(words) - win_size):
            target = word_to_idx[words[i]]
            context_x=[word_to_idx[words[i-1]],word_to_idx[words[i+1]]]
            x.append(np.array(context_x))
            y.append(target)
            
    x = np.array(x)
    y = np.array(y)
    return x,y

class Generator(keras.utils.Sequence):
    vocab_size = 35527
    win_size=1
    def __init__(self, x_set, y_set, batch_size=4):
        self.x= x_set
        self.y =y_set
        self.batch_size = batch_size
        self.indices = np.arange(len(self.x))

    def __len__(self):
        return int(np.ceil(len(self.x) / self.batch_size))

    def __getitem__(self, idx):
        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_x, batch_y = [], []
        for ind in inds:
            batch_x.append(np.array(self.x[ind]))
            hot_encoded = []
            temp_y = to_categorical(self.y[ind], num_classes=vocab_size)
            batch_y.append(temp_y)
        batch_x = np.array(batch_x)
        batch_y = np.array(batch_y)

        return batch_x, batch_y

def get_model_cbow(vocab_size):
    win_size=1
    embedding_dim = 50
    model = Sequential()
    model.add(Embedding(vocab_size, embedding_dim, input_shape=(2,)))
    model.add(Flatten())
    activations='relu'
    model.add(Dense(512,activation=activations))
    activations='softmax'
    model.add(Dense(vocab_size, activation=activations))
    model.compile(loss='binary_crossentropy',
                optimizer=Adam(0.01),
                metrics=['accuracy'])
    
    return model

def get_model_negative(vocab_size):
    win_size=1
    embedding_dim = 50
    model = Sequential()
    model.add(Embedding(vocab_size, embedding_dim, embeddings_initializer="RandomNormal"))
    model.add(GlobalAveragePooling1D())
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy',
                optimizer=Adam(0.01),
                metrics=['accuracy'])
    
    return model

vocab_size = len(word2idx)
vocab_size

train_x,train_y=get_train_x_y(data,word2idx)

batch_size = 4096
my_training_batch_generator = Generator(train_x, train_y, batch_size)
train_rows,train_cols = train_x.shape

model = get_model_cbow(vocab_size)

model.fit(my_training_batch_generator,
                   steps_per_epoch = int(train_rows // batch_size),
                   epochs = 3,
                   verbose = 1,
                   max_queue_size=15)

model_cbow_json = model.to_json()
with open("model_cbow.json", "w") as json_file:
    json_file.write(model_cbow_json)

model.save_weights("model_cbow.h5")
# print("Saved model to disk")

cbow_matrix_weights_1 = model.layers[0].get_weights()[0]

positive_sample=get_positive_samples(data,word2idx)
# positive_sample

n_positive_samples = len(positive_sample)
temp_n = 0.02*n_positive_samples
n_negative_samples = 0
# temp_n

positive_sample_set = set()
for i in positive_sample:
    positive_sample_set.add(tuple(i))

negative_samples = []
while n_negative_samples < temp_n:
    temp_neg_samples = get_negative_samples(word2idx)
    if tuple(temp_neg_samples) not in positive_sample_set:
        negative_samples.append(temp_neg_samples)
        n_negative_samples = n_negative_samples+1

dict_dump = {"neg_sample":  negative_samples}
fileObj = open('data_neg_sample.obj', 'wb')
pickle.dump(dict_dump,fileObj)
fileObj.close()

fileObj = open('/content/drive/MyDrive/nlp/data_neg_sample.obj', 'rb')
dump_dict = pickle.load(fileObj)
negative_samples = dump_dict['neg_sample']
n_negative_samples = len(negative_samples)

X = np.concatenate([np.array(positive_sample), np.array(negative_samples)], axis=0)
y = np.concatenate([[1]*n_positive_samples, [0]*n_negative_samples])

cbow_neg_model = get_model_negative(vocab_size)

cbow_neg_model.fit(X,y,validation_split=0.15, batch_size=256, epochs=10)

model_cbow_neg_json = cbow_neg_model.to_json()
with open("/content/drive/MyDrive/nlp/model_neg_cbow.json", "w") as json_file:
    json_file.write(model_cbow_neg_json)

model_cbow_neg_json.save_weights("model_cbow_neg_sample.h5")
print("Saved model to disk")

cbow_matrix_weights = cbow_neg_model.layers[0].get_weights()[0]

dict_dump = {"embedding":  cbow_matrix_weights}
fileObj = open('cbow_neg_sample_embedding.obj', 'wb')
pickle.dump(dict_dump,fileObj)
fileObj.close()

matrix=cbow_matrix_weights

word = ['titanic']
plot_2D(word)
# cbow_matrix_weights

word = ['man']
plot_2D(word)

word = ['daughter']
plot_2D(word)

word = ['is']
plot_2D(word)

word = ['camera']
plot_2D(word)

word = ['camera','is','titanic','man','daughter']
plot_2D(word)

def plot_2D(words):

  plt.figure(figsize=(20, 20)) 

  colors = cm.rainbow(np.linspace(0, 1, 5))
  # 5 will be no of words we are given for each colour we will have different colors

  for word, c in zip(words, colors):
    index_of_word=vocab.index(word)
    embedding=np.array([matrix[index_of_word]])
    # print(embedding.shape)
    all_words=list()
    all_words.append(word)

    top_ten_similar_word=top_ten_words(word)

    for i in top_ten_similar_word:
      all_words.append(i[0])
      index_of_similar_word=vocab.index(i[0])
      embedding=np.append(embedding,[matrix[index_of_similar_word]],axis=0)
    
    
    # print(embedding.shape)

    
    X_embedding=TSNE(n_components=2, learning_rate='auto',init='pca', perplexity=5).fit_transform(embedding)
    
    
    # two=X_embedding[:,:2]
    # print(two)
    
    
    print(all_words)
    plt.scatter(X_embedding[:, 0], X_embedding[:, 1], color = c)

    for i in range(0,len(all_words)):
      plt.annotate(all_words[i], xy=(X_embedding[i, 0], X_embedding[i, 1]), xytext=(5, 2), 
                 textcoords='offset points', 
                 ha='right', 
                 va='bottom')
    plt.title('t-SNE plot of high-dimensional data of ')
    
    # for word, (x,y) in zip(top_ten_similar_word, twodim):

word = ['titanic','house','run','amazing','has']
plot_2D(word)

"""
**Word2Vec PreTrained**"""

!pip install --upgrade gensim

from gensim.models import KeyedVectors

# path = '/content/drive/MyDrive/nlp'
word2vec = KeyedVectors.load_word2vec_format("/content/drive/MyDrive/GoogleNews-vectors-negative300.bin", binary=True)

def print_plot_word2vec(word, k,word2vec):
  neighbor_words = word2vec.most_similar(word, topn=k)
  names, weights = [], []
  names.append(word)
  weights.append(word2vec[word])

  for word,_ in neighbor_words:
    names.append(word)
    weights.append(word2vec[word])
  weights = np.array(weights)
  draw_plot(names,weights)

word = 'titanic'
k = 10
print_plot_word2vec(word,k,word2vec)

word = 'camera'
print_plot_word2vec(word,k,word2vec)